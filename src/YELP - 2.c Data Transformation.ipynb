{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6df4b689-ce5e-46a7-b4f2-2e14d5136eb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Yelp Data Transformation - Overview\n",
    "\n",
    "\n",
    "## Objective\n",
    "\n",
    "The transformation process aims to:\n",
    "\n",
    "- **Improve Data Quality**: Handle missing values, normalize formats, and ensure integrity.\n",
    "- **Standardize Data**: Convert date formats, structure text fields, and normalize attributes.\n",
    "- **Enhance Features**: Extract insights from review lengths and sentiment scores.\n",
    "- **Optimize Performance**: Reduce storage costs and improve query speed.\n",
    "- **Ensure Scalability**: Support distributed processing for large-scale datasets.\n",
    "- **Enable Monitoring**: Implement logging to track errors and measure efficiency.\n",
    "- **Use Delta Lake**: Convert Parquet to Delta for reliability, ACID transactions, and optimized queries.\n",
    "\n",
    "\n",
    "## Parquet to Delta Conversion\n",
    "\n",
    "### **Objective:**\n",
    "Converting Parquet files to **Delta format** improves **query performance, reliability, and incremental processing.**\n",
    "\n",
    "### **Steps:**\n",
    "\n",
    "##### **1. Load Parquet Data**\n",
    "```python\n",
    "parquet_df = spark.read.parquet(\"/data/yelp/parquet/business\")\n",
    "```\n",
    "##### **2. Write as Delta Table**\n",
    "```python\n",
    "parquet_df.write.format(\"delta\").mode(\"overwrite\").save(\"/data/yelp/delta/business\")\n",
    "```\n",
    "##### **3. Register Delta Table**\n",
    "```python\n",
    "spark.sql(\"CREATE TABLE yelp_analytics.dim_business USING DELTA LOCATION '/data/yelp/delta/business'\")\n",
    "```\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/kdMhfbw/flow-transformation.png\" alt=\"flow-transformation\" border=\"0\"></a><br /><a target='_blank' href='https://imgbb.com/'>transformation</a><br />\n",
    "## **Handling Peak and Off-Peak Load Considerations**\n",
    "```python\n",
    "if load_condition == \"peak\":\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", \"500\")\n",
    "    print(f\"âš¡ Peak Load for {table_name} ({file_size_mb:.2f} MB). Adjusting resources.\")\n",
    "else:\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", \"100\")\n",
    "    print(f\"ðŸŒ™ Off-Peak Load for {table_name} ({file_size_mb:.2f} MB). Optimizing resources.\")\n",
    "```\n",
    "**Explanation:**\n",
    "- Dynamically **adjusts computing power** based on **file size**.\n",
    "- **Peak load**: Uses **500 partitions** to process large files efficiently.\n",
    "- **Off-peak**: Uses **100 partitions** to conserve resources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "979357f8-2e52-44ff-9658-49bbc1e168ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Init Funtion for Transformation in Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a847de2-4559-4e5a-85fd-52775154e97f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import current_date, lit, col\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"IncrementalELT\").getOrCreate()\n",
    "\n",
    "# Base Parquet directory (update as needed)\n",
    "parquet_base_path = \"/data/yelp/parquet/\"\n",
    "\n",
    "# Function to get Parquet size (in MB)\n",
    "def get_parquet_size(path):\n",
    "    try:\n",
    "        hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "        path_obj = spark._jvm.org.apache.hadoop.fs.Path(path)\n",
    "        fs = path_obj.getFileSystem(hadoop_conf)\n",
    "        size_bytes = fs.getContentSummary(path_obj).getLength()\n",
    "        return size_bytes / (1024 * 1024)  # Convert bytes to MB\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Warning: Unable to retrieve size for {path}. {e}\")\n",
    "        return 0\n",
    "\n",
    "# Table-specific peak threshold (1GB per table)\n",
    "PEAK_THRESHOLD_MB = 1000  \n",
    "\n",
    "# Logging function to track ELT execution\n",
    "def log_elt_process(table_name, status, exec_time, rows_affected=0, error_message=None, method_used=\"DataFrame\", size=\"unknown\"):\n",
    "    new_log_id = spark.sql(\"SELECT COALESCE(MAX(log_id), 0) + 1 FROM config_db.elt_process_log\").collect()[0][0]\n",
    "    current_ts = datetime.now()\n",
    "    log_sql = f\"\"\"\n",
    "        INSERT INTO config_db.elt_process_log \n",
    "        (log_id, process_name, target_table, start_time, end_time, execution_time_seconds, size, rows_affected, method_used, status, error_message) \n",
    "        VALUES ({new_log_id}, 'Incremental Load', '{table_name}', '{current_ts}', '{current_ts}', {exec_time}, '{size}', {rows_affected}, '{method_used}', '{status}', {f'\"{error_message}\"' if error_message else 'NULL'})\n",
    "    \"\"\"\n",
    "    print(f\"\"\"{status} {method_used} {exec_time}\"\"\")\n",
    "    if status== \"Failed\": print(log_sql)\n",
    "    spark.sql(log_sql)\n",
    "\n",
    "def get_readable_size(size_bytes):\n",
    "    \"\"\"Convert bytes to a human-readable MB format.\"\"\"\n",
    "    return f\"{size_bytes / (1024*1024):.2f} MB\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "281f0422-d7e9-4df5-86ad-e1eb0d431985",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Business Dimension (`dim_business`)**\n",
    "```python\n",
    "source_df = (spark.read.parquet(\"/data/yelp/parquet/business\")\n",
    "                  .select(\"business_id\", \"name\", \"address\", \"city\", \"state\", \"postal_code\", \"stars\", \"review_count\", \"is_open\")\n",
    "                  .withColumn(\"effective_date\", current_date()))\n",
    "```\n",
    "**Explanation:**\n",
    "- Loads business details including location, rating, and status.\n",
    "- Adds `effective_date` for historical tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bbe3bcb-8d3e-45b1-bfb4-c3745c689098",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ™ Off-Peak Load for dim_business (17.17 MB). Optimizing resources.\n\n        INSERT INTO config_db.elt_process_log\n        (log_id, process_name, target_table, start_time, end_time, execution_time_seconds, size, rows_affected, method_used, status, error_message)\n        VALUES (14, 'Incremental Load', 'dim_business', '2025-02-24 21:42:12.056553', '2025-02-24 21:42:12.056553', 0, '17.17 MB', 150346, 'DataFrame (Off-peak)', 'Success', \"20.747179\")\n    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# Incremental Load for dim_business\n",
    "# ---------------------------\n",
    "try:\n",
    "    table_name = \"dim_business\"\n",
    "    start = datetime.now()\n",
    "    source_path = parquet_base_path + \"business\"\n",
    "    \n",
    "    # Get Parquet file size for the table\n",
    "    file_size_mb = get_parquet_size(source_path)\n",
    "    \n",
    "    # Determine load condition based on table size\n",
    "    load_condition = \"peak\" if file_size_mb > PEAK_THRESHOLD_MB else \"off-peak\"\n",
    "\n",
    "    # Adjust Spark settings dynamically\n",
    "    if load_condition == \"peak\":\n",
    "        spark.conf.set(\"spark.sql.shuffle.partitions\", \"500\")\n",
    "        print(f\"âš¡ Peak Load for {table_name} ({file_size_mb:.2f} MB). Adjusting resources.\")\n",
    "    else:\n",
    "        spark.conf.set(\"spark.sql.shuffle.partitions\", \"100\")\n",
    "        print(f\"ðŸŒ™ Off-Peak Load for {table_name} ({file_size_mb:.2f} MB). Optimizing resources.\")\n",
    "\n",
    "    # Read Parquet data\n",
    "    source_df = spark.read.parquet(source_path).selectExpr(\n",
    "        \"business_id\", \"name\", \"address\", \"city\", \"state\", \"postal_code\", \"latitude\", \"longitude\",\n",
    "        \"is_open\", \"categories\", \"review_count\", \"stars\", \"current_date() as effective_date\"\n",
    "    )\n",
    "    rows_count = source_df.count()\n",
    "\n",
    "    # Merge with Delta Table\n",
    "    target = DeltaTable.forName(spark, \"yelp_analytics.dim_business\")\n",
    "    merge_cond = \"target.business_id = source.business_id AND target.current_flag = true\"\n",
    "    update_cond = \"\"\"target.name <> source.name OR target.address <> source.address OR\n",
    "                      target.city <> source.city OR target.state <> source.state OR\n",
    "                      target.postal_code <> source.postal_code OR target.latitude <> source.latitude OR\n",
    "                      target.longitude <> source.longitude OR target.is_open <> source.is_open OR\n",
    "                      target.categories <> source.categories OR target.review_count <> source.review_count OR\n",
    "                      target.stars <> source.stars\"\"\"\n",
    "    target.alias(\"target\").merge(\n",
    "        source_df.alias(\"source\"), merge_cond\n",
    "    ).whenMatchedUpdate(\n",
    "        condition=update_cond,\n",
    "        set={\"expiry_date\": \"current_date()\", \"current_flag\": \"false\"}\n",
    "    ).whenNotMatchedInsert(\n",
    "        values={\n",
    "            \"business_id\": \"source.business_id\",\n",
    "            \"name\": \"source.name\",\n",
    "            \"address\": \"source.address\",\n",
    "            \"city\": \"source.city\",\n",
    "            \"state\": \"source.state\",\n",
    "            \"postal_code\": \"source.postal_code\",\n",
    "            \"latitude\": \"source.latitude\",\n",
    "            \"longitude\": \"source.longitude\",\n",
    "            \"is_open\": \"source.is_open\",\n",
    "            \"categories\": \"source.categories\",\n",
    "            \"review_count\": \"source.review_count\",\n",
    "            \"stars\": \"source.stars\",\n",
    "            \"effective_date\": \"source.effective_date\",\n",
    "            \"expiry_date\": lit(\"9999-12-31\").cast(\"date\"),\n",
    "            \"current_flag\": lit(True)\n",
    "        }\n",
    "    ).execute()\n",
    "\n",
    "    exec_time = (datetime.now() - start).total_seconds()\n",
    "    \n",
    "    # Log with Peak/Off-Peak information\n",
    "    log_elt_process(table_name, \"Success\", exec_time, rows_affected=rows_count, \n",
    "                    size=f\"{file_size_mb:.2f} MB\", method_used=f\"DataFrame ({load_condition.capitalize()})\")\n",
    "except Exception as e:\n",
    "    exec_time = (datetime.now() - start).total_seconds()\n",
    "    log_elt_process(table_name, \"Failed\", exec_time, rows_affected=0, size=\"unknown\", \n",
    "                    error_message=str(e), method_used=f\"DataFrame ({load_condition.capitalize()})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e911b884-1d8b-4418-a69b-f701ec12a241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### **User Dimension (`dim_user`)**\n",
    "```python\n",
    "source_df = (spark.read.parquet(\"/data/yelp/parquet/user\")\n",
    "                  .select(\"user_id\", \"name\", \"yelping_since\", \"review_count\", \"average_stars\", \"fans\", \"cool\", \"funny\", \"useful\", \"elite\")\n",
    "                  .withColumn(\"effective_date\", current_date())\n",
    "                  .withColumn(\"cool\", col(\"cool\").cast(\"integer\"))\n",
    "                  .withColumn(\"funny\", col(\"funny\").cast(\"integer\"))\n",
    "                  .withColumn(\"useful\", col(\"useful\").cast(\"integer\")))\n",
    "```\n",
    "**Explanation:**\n",
    "- Reads the `user` data from Parquet files.\n",
    "- Selects only relevant attributes.\n",
    "- Converts numerical attributes to proper types.\n",
    "- Adds `effective_date` to track changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b788169-8a09-4d7b-811d-c2c8671d882d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ Peak Load for dim_user (2496.88 MB). Adjusting resources.\n\n        INSERT INTO config_db.elt_process_log\n        (log_id, process_name, target_table, start_time, end_time, execution_time_seconds, size, rows_affected, method_used, status, error_message)\n        VALUES (13, 'Incremental Load', 'dim_user', '2025-02-24 21:05:46.611369', '2025-02-24 21:05:46.611369', 0, '2496.88 MB', 1987897, 'DataFrame (peak)', 'Success', \"33.03772\")\n    \n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Incremental Load for dim_user (SCD Type 2)\n",
    "# ---------------------------\n",
    "try:\n",
    "    table_name = \"dim_user\"\n",
    "    start = datetime.now()\n",
    "    source_path = f\"{parquet_base_path}user\"\n",
    "\n",
    "    # Determine Load Condition (Peak or Off-Peak)\n",
    "    file_size_mb = get_parquet_size(source_path)\n",
    "    load_condition = \"peak\" if file_size_mb > PEAK_THRESHOLD_MB else \"off-peak\"\n",
    "\n",
    "    # Adjust Spark settings dynamically\n",
    "    if load_condition == \"peak\":\n",
    "        spark.conf.set(\"spark.sql.shuffle.partitions\", \"500\")\n",
    "        print(f\"âš¡ Peak Load for {table_name} ({file_size_mb:.2f} MB). Adjusting resources.\")\n",
    "    else:\n",
    "        spark.conf.set(\"spark.sql.shuffle.partitions\", \"100\")\n",
    "        print(f\"ðŸŒ™ Off-Peak Load for {table_name} ({file_size_mb:.2f} MB). Optimizing resources.\")\n",
    "\n",
    "    # Read new data\n",
    "    source_df = (spark.read.parquet(source_path)\n",
    "                      .selectExpr(\n",
    "                          \"user_id\", \"name\", \"yelping_since\", \"review_count\", \"average_stars\",\n",
    "                          \"fans\", \"cool\", \"funny\", \"useful\", \"elite\"\n",
    "                      )\n",
    "                      .withColumn(\"cool\", col(\"cool\").cast(\"integer\"))\n",
    "                      .withColumn(\"funny\", col(\"funny\").cast(\"integer\"))\n",
    "                      .withColumn(\"useful\", col(\"useful\").cast(\"integer\"))\n",
    "                      .withColumn(\"effective_date\", current_date())  # Set current date as `effective_date`\n",
    "                )\n",
    "\n",
    "    rows_count = source_df.count()\n",
    "\n",
    "    # Reference Delta Table\n",
    "    target = DeltaTable.forName(spark, f\"yelp_analytics.{table_name}\")\n",
    "    merge_cond = \"target.user_id = source.user_id AND target.current_flag = true\"\n",
    "\n",
    "    # Define Update Conditions (Detect Changes)\n",
    "    update_cond = \"\"\"target.name <> source.name OR\n",
    "                      target.yelping_since <> source.yelping_since OR\n",
    "                      target.review_count <> source.review_count OR\n",
    "                      target.average_stars <> source.average_stars OR\n",
    "                      target.fans <> source.fans OR\n",
    "                      target.cool <> source.cool OR\n",
    "                      target.funny <> source.funny OR\n",
    "                      target.useful <> source.useful OR\n",
    "                      target.elite <> source.elite\"\"\"\n",
    "\n",
    "    # Merge Operation with SCD Type 2 Logic\n",
    "    target.alias(\"target\").merge(\n",
    "        source_df.alias(\"source\"), merge_cond\n",
    "    ).whenMatchedUpdate(\n",
    "        condition=update_cond,\n",
    "        set={\"expiry_date\": \"current_date()\", \"current_flag\": \"false\"}  # Expire old record\n",
    "    ).whenNotMatchedInsert(\n",
    "        values={\n",
    "            \"user_id\": \"source.user_id\",\n",
    "            \"name\": \"source.name\",\n",
    "            \"yelping_since\": \"source.yelping_since\",\n",
    "            \"review_count\": \"source.review_count\",\n",
    "            \"average_stars\": \"source.average_stars\",\n",
    "            \"fans\": \"source.fans\",\n",
    "            \"cool\": \"source.cool\",\n",
    "            \"funny\": \"source.funny\",\n",
    "            \"useful\": \"source.useful\",\n",
    "            \"elite\": \"source.elite\",\n",
    "            \"effective_date\": \"source.effective_date\",\n",
    "            \"expiry_date\": lit(\"9999-12-31\").cast(\"date\"),  # Default future expiry\n",
    "            \"current_flag\": lit(True)\n",
    "        }\n",
    "    ).execute()\n",
    "\n",
    "    exec_time = (datetime.now() - start).total_seconds()\n",
    "\n",
    "    # Log ELT Execution\n",
    "    log_elt_process(table_name, \"Success\", exec_time, rows_affected=rows_count, \n",
    "                    size=f\"{file_size_mb:.2f} MB\", method_used=f\"DataFrame ({load_condition})\")\n",
    "\n",
    "except Exception as e:\n",
    "    exec_time = (datetime.now() - start).total_seconds()\n",
    "    log_elt_process(table_name, \"Failed\", exec_time, rows_affected=0, size=\"unknown\", \n",
    "                    error_message=str(e), method_used=f\"DataFrame ({load_condition})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b8747aa-d61a-4f13-95de-1cee349d7aa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Review Fact Table (`fact_review`)**\n",
    "```python\n",
    "source_df = (spark.read.parquet(\"/data/yelp/parquet/review\")\n",
    "                  .select(\"review_id\", \"user_id\", \"business_id\", \"stars\", \"date\", \"cool\", \"funny\", \"useful\")\n",
    "                  .withColumn(\"review_date\", col(\"date\").cast(\"date\"))\n",
    "                  .withColumn(\"cool\", col(\"cool\").cast(\"integer\"))\n",
    "                  .withColumn(\"funny\", col(\"funny\").cast(\"integer\"))\n",
    "                  .withColumn(\"useful\", col(\"useful\").cast(\"integer\")))\n",
    "```\n",
    "**Explanation:**\n",
    "- Loads the review data and casts columns for correct data types.\n",
    "- Adds `review_date` for time-based analytics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "202e65e5-1097-4297-80e5-3546d3d57aa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ Peak Load for fact_review (2848.79 MB). Adjusting resources.\nSuccess DataFrame (Peak) 86.702651\nðŸŽ‰ Incremental ELT process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# Incremental Load for fact_review\n",
    "# ---------------------------\n",
    "try:\n",
    "    table_name = \"fact_review\"\n",
    "    start = datetime.now()\n",
    "    source_path = parquet_base_path + \"review\"\n",
    "    \n",
    "    file_size_mb = get_parquet_size(source_path)\n",
    "    load_condition = \"peak\" if file_size_mb > PEAK_THRESHOLD_MB else \"off-peak\"\n",
    "\n",
    "    if load_condition == \"peak\":\n",
    "        spark.conf.set(\"spark.sql.shuffle.partitions\", \"500\")\n",
    "        print(f\"âš¡ Peak Load for {table_name} ({file_size_mb:.2f} MB). Adjusting resources.\")\n",
    "    else:\n",
    "        spark.conf.set(\"spark.sql.shuffle.partitions\", \"100\")\n",
    "        print(f\"ðŸŒ™ Off-Peak Load for {table_name} ({file_size_mb:.2f} MB). Optimizing resources.\")\n",
    "\n",
    "    max_date = spark.sql(\"SELECT COALESCE(MAX(review_date), '1900-01-01') as max_date FROM yelp_analytics.fact_review\").collect()[0][0]\n",
    "\n",
    "    source_df = (spark.read.parquet(source_path)\n",
    "                      .filter(col(\"date\") > lit(max_date))\n",
    "                      .withColumn(\"review_date\", col(\"date\").cast(\"date\"))\n",
    "                      .withColumn(\"cool\", col(\"cool\").cast(\"integer\"))\n",
    "                      .withColumn(\"funny\", col(\"funny\").cast(\"integer\"))\n",
    "                      .withColumn(\"useful\", col(\"useful\").cast(\"integer\"))\n",
    "                 )\n",
    "    source_df = source_df.select(\"review_id\", \"business_id\", \"user_id\", \"review_date\", \"stars\", \"cool\", \"funny\", \"useful\")\n",
    "    \n",
    "    rows_count = source_df.count()\n",
    "    \n",
    "    source_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"yelp_analytics.fact_review\")\n",
    "    exec_time = (datetime.now() - start).total_seconds()\n",
    "\n",
    "    log_elt_process(table_name, \"Success\", exec_time, rows_affected=rows_count, size=f\"{file_size_mb:.2f} MB\",\n",
    "                    method_used=f\"DataFrame ({load_condition.capitalize()})\")\n",
    "except Exception as e:\n",
    "    exec_time = (datetime.now() - start).total_seconds()\n",
    "    log_elt_process(table_name, \"Failed\", exec_time, rows_affected=0, size=\"unknown\", \n",
    "                    error_message=str(e), method_used=f\"DataFrame ({load_condition.capitalize()})\")\n",
    "\n",
    "print(\"ðŸŽ‰ Incremental ELT process completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5209b6e7-7640-45b0-84af-ee79d6c781df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Check-in Fact Table (`fact_checkin`)**\n",
    "```python\n",
    "source_df = (spark.read.parquet(\"/data/yelp/parquet/checkin\")\n",
    "                  .select(\"business_id\", col(\"date\").cast(\"date\").alias(\"checkin_date\")))\n",
    "```\n",
    "**Explanation:**\n",
    "- Extracts business check-ins and timestamps them for trend analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c080793-71f6-421e-9d2d-91f83f65fb2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ™ Off-Peak Load for fact_checkin (0.00 MB). Optimizing resources.\nSuccess DataFrame (off-peak) 12.5958\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# Incremental Load for fact_checkin\n",
    "# ---------------------------\n",
    "try:\n",
    "    start = datetime.now()\n",
    "    table_name = \"fact_checkin\"\n",
    "    source_path = \"/data/yelp/parquet/checkin\"\n",
    "\n",
    "    file_size_mb = get_parquet_size(source_path) / (1024 * 1024)\n",
    "    load_condition = \"peak\" if file_size_mb > PEAK_THRESHOLD_MB else \"off-peak\"\n",
    "\n",
    "    # **Adjust Spark settings dynamically based on load condition**\n",
    "    if load_condition == \"peak\":\n",
    "        spark.conf.set(\"spark.sql.shuffle.partitions\", \"500\")\n",
    "        print(f\"âš¡ Peak Load for {table_name} ({file_size_mb:.2f} MB). Adjusting resources.\")\n",
    "    else:\n",
    "        spark.conf.set(\"spark.sql.shuffle.partitions\", \"100\")\n",
    "        print(f\"ðŸŒ™ Off-Peak Load for {table_name} ({file_size_mb:.2f} MB). Optimizing resources.\")\n",
    "\n",
    "    max_date = spark.sql(\"SELECT COALESCE(MAX(checkin_date), '1900-01-01') as max_date FROM yelp_analytics.fact_checkin\").collect()[0][0]\n",
    "    source_df = spark.read.parquet(source_path).filter(col(\"date\") > lit(max_date))\n",
    "\n",
    "    # **Ensure schema consistency**\n",
    "    source_df = source_df.select(\"business_id\", col(\"date\").cast(\"date\").alias(\"checkin_date\"))\n",
    "\n",
    "    rows_count = source_df.count()\n",
    "    file_size = get_readable_size(get_parquet_size(source_path))\n",
    "\n",
    "    # **Enable schema auto-merge**\n",
    "    source_df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\"yelp_analytics.fact_checkin\")\n",
    "\n",
    "    exec_time = (datetime.now() - start).total_seconds()\n",
    "    log_elt_process(table_name, \"Success\", exec_time, rows_affected=rows_count, size=file_size, method_used=f\"DataFrame ({load_condition})\")\n",
    "except Exception as e:\n",
    "    exec_time = (datetime.now() - start).total_seconds()\n",
    "    log_elt_process(table_name, \"Failed\", exec_time, rows_affected=rows_count if 'rows_count' in locals() else 0,\n",
    "                    size=file_size if 'file_size' in locals() else \"unknown\", error_message=str(e), method_used=f\"DataFrame ({load_condition})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ea2448d-a385-455e-a4de-06c0b3f219ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Tip Fact Table (`fact_tip`)**\n",
    "```python\n",
    "source_df = (spark.read.parquet(\"/data/yelp/parquet/tip\")\n",
    "                  .select(\"business_id\", \"user_id\", col(\"date\").cast(\"date\").alias(\"tip_date\"), \"text\", col(\"compliment_count\").cast(\"integer\")))\n",
    "```\n",
    "**Explanation:**\n",
    "- Processes user tips with timestamps for insights on feedback trends.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41b1f67b-246e-4fe7-b62d-bee96ba28467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ™ Off-Peak Load for fact_tip (0.00 MB). Optimizing resources.\nSuccess DataFrame (off-peak) 19.012576\nðŸŽ‰ Incremental ELT process completed.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Incremental Load for fact_tip\n",
    "# ---------------------------\n",
    "try:\n",
    "    start = datetime.now()\n",
    "    table_name = \"fact_tip\"\n",
    "    source_path = \"/data/yelp/parquet/tip\"\n",
    "\n",
    "    file_size_mb = get_parquet_size(source_path) / (1024 * 1024)\n",
    "    load_condition = \"peak\" if file_size_mb > PEAK_THRESHOLD_MB else \"off-peak\"\n",
    "\n",
    "    # **Adjust Spark settings dynamically based on load condition**\n",
    "    if load_condition == \"peak\":\n",
    "        spark.conf.set(\"spark.sql.shuffle.partitions\", \"500\")\n",
    "        print(f\"âš¡ Peak Load for {table_name} ({file_size_mb:.2f} MB). Adjusting resources.\")\n",
    "    else:\n",
    "        spark.conf.set(\"spark.sql.shuffle.partitions\", \"100\")\n",
    "        print(f\"ðŸŒ™ Off-Peak Load for {table_name} ({file_size_mb:.2f} MB). Optimizing resources.\")\n",
    "\n",
    "    max_date = spark.sql(\"SELECT COALESCE(MAX(tip_date), '1900-01-01') as max_date FROM yelp_analytics.fact_tip\").collect()[0][0]\n",
    "    source_df = spark.read.parquet(source_path).filter(col(\"date\") > lit(max_date))\n",
    "\n",
    "    # **Ensure schema consistency**\n",
    "    source_df = source_df.select(\n",
    "        \"business_id\", \"user_id\",\n",
    "        col(\"date\").cast(\"date\").alias(\"tip_date\"),\n",
    "        \"text\", col(\"compliment_count\").cast(\"integer\")\n",
    "    )\n",
    "\n",
    "    rows_count = source_df.count()\n",
    "    file_size = get_readable_size(get_parquet_size(source_path))\n",
    "\n",
    "    # **Enable schema auto-merge**\n",
    "    source_df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\"yelp_analytics.fact_tip\")\n",
    "\n",
    "    exec_time = (datetime.now() - start).total_seconds()\n",
    "    log_elt_process(table_name, \"Success\", exec_time, rows_affected=rows_count, size=file_size, method_used=f\"DataFrame ({load_condition})\")\n",
    "except Exception as e:\n",
    "    exec_time = (datetime.now() - start).total_seconds()\n",
    "    log_elt_process(table_name, \"Failed\", exec_time, rows_affected=rows_count if 'rows_count' in locals() else 0,\n",
    "                    size=file_size if 'file_size' in locals() else \"unknown\", error_message=str(e), method_used=f\"DataFrame ({load_condition})\")\n",
    "\n",
    "print(\"ðŸŽ‰ Incremental ELT process completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a9fa8c3-649c-4560-9fb4-01bad38deace",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Rising Star Business - SCD Type 3 Implementation**\n",
    "\n",
    "#### **Objective:**\n",
    "The **Rising Star Business** transformation follows **Slowly Changing Dimension (SCD) Type 3**, which tracks both historical and current business performance trends. This allows comparisons of past and present performance without losing previous data.\n",
    "\n",
    "#### **Transformation Steps:**\n",
    "\n",
    "##### **Step 1: Compute Business Rating Trends**\n",
    "```python\n",
    "fact_review_df = spark.table(\"yelp_analytics.fact_review\")\n",
    "\n",
    "review_agg_df = fact_review_df.withColumn(\"review_year\", year(col(\"review_date\")))\\\n",
    "    .groupBy(\"business_id\", \"review_year\")\\\n",
    "    .agg(count(\"*\").alias(\"review_count\"), avg(\"stars\").alias(\"avg_rating\"))\n",
    "```\n",
    "**Explanation:**\n",
    "- Extracts **yearly review trends** per business.\n",
    "- Computes **review count** and **average rating**.\n",
    "\n",
    "##### **Step 2: Identify Rising Stars**\n",
    "```python\n",
    "curr_df = review_agg_df.alias(\"curr\")\n",
    "prev_df = review_agg_df.alias(\"prev\")\n",
    "\n",
    "rising_stars_df = curr_df.join(\n",
    "    prev_df, \n",
    "    (col(\"curr.business_id\") == col(\"prev.business_id\")) & (col(\"curr.review_year\") == col(\"prev.review_year\") + 1)\n",
    ").where(\n",
    "    (col(\"curr.review_count\") >= 10) & (col(\"curr.avg_rating\") >= col(\"prev.avg_rating\") + 1)\n",
    ")\n",
    "```\n",
    "**Explanation:**\n",
    "- Joins **consecutive year data** to identify businesses with improving ratings.\n",
    "- Filters businesses with **at least 10 reviews** and a **1-star improvement**.\n",
    "\n",
    "##### **Step 3: Merge Data into SCD Type 3 Table**\n",
    "```python\n",
    "target_table = DeltaTable.forName(spark, \"yelp_analytics.datamart_rising_star_businesses\")\n",
    "merge_condition = \"target.business_id = source.business_id\"\n",
    "\n",
    "target_table.alias(\"target\").merge(\n",
    "    rising_stars_df.alias(\"source\"),\n",
    "    merge_condition\n",
    ").whenMatchedUpdate(\n",
    "    set={\n",
    "        \"previous_avg_rating\": \"target.current_avg_rating\",\n",
    "        \"current_avg_rating\": \"source.avg_rating\",\n",
    "        \"review_count\": \"source.review_count\",\n",
    "        \"last_update_date\": \"current_date()\"\n",
    "    }\n",
    ").whenNotMatchedInsert(\n",
    "    values={\n",
    "        \"business_id\": \"source.business_id\",\n",
    "        \"previous_avg_rating\": \"NULL\",\n",
    "        \"current_avg_rating\": \"source.avg_rating\",\n",
    "        \"review_count\": \"source.review_count\",\n",
    "        \"last_update_date\": \"current_date()\"\n",
    "    }\n",
    ").execute()\n",
    "```\n",
    "**Explanation:**\n",
    "- Uses **SCD Type 3**, where `previous_avg_rating` stores the last known value.\n",
    "- **On update:** Moves `current_avg_rating` to `previous_avg_rating` and updates with the new value.\n",
    "- **On insert:** Sets `previous_avg_rating` as NULL (first-time entry).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ce22fa0-0f05-4680-982f-94d3f8694021",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ™ Off-Peak Load for fact_review (356.17 MB). Optimizing resource usage.\n\n        INSERT INTO config_db.elt_process_log\n        (log_id, process_name, target_table, start_time, end_time, execution_time_seconds, size, rows_affected, method_used, status, error_message)\n        VALUES (12, 'Incremental Load', 'datamart_rising_star_businesses', '2025-02-24 19:39:29.136763', '2025-02-24 19:39:29.136763', 34.92769, '356.17 MB', 0, 'DataFrame (Off-peak)', 'Success', NULL)\n    \n"
     ]
    }
   ],
   "source": [
    "# Function to get table size in MB\n",
    "def get_table_size(table_name):\n",
    "    try:\n",
    "        size_bytes = spark.sql(f\"DESCRIBE DETAIL {table_name}\").select(\"sizeInBytes\").collect()[0][0]\n",
    "        size_mb = size_bytes / (1024 * 1024)  # Convert bytes to MB\n",
    "        return size_mb\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Warning: Unable to retrieve size for {table_name}. {e}\")\n",
    "        return 0  # Default to 0MB if error occurs\n",
    "\n",
    "# Peak Threshold (1GB per table)\n",
    "PEAK_THRESHOLD_MB = 1000  \n",
    "\n",
    "# Enhanced logging function\n",
    "def log_elt_process(table_name, status, error_message=None, execution_time_seconds=0, size=\"unknown\", rows_affected=0, method_used=\"SQL\"):\n",
    "    log_id = spark.sql(\"SELECT COALESCE(MAX(log_id), 0) + 1 FROM config_db.elt_process_log\").collect()[0][0]\n",
    "    current_time = datetime.now()\n",
    "    sql_log = f\"\"\"\n",
    "        INSERT INTO config_db.elt_process_log\n",
    "        (log_id, process_name, target_table, start_time, end_time, execution_time_seconds, size, rows_affected, method_used, status, error_message)\n",
    "        VALUES ({log_id}, 'Incremental Load', '{table_name}', '{current_time}', '{current_time}', {execution_time_seconds}, '{size}', {rows_affected}, '{method_used}', '{status}', {f'\"{error_message}\"' if error_message else 'NULL'})\n",
    "    \"\"\"\n",
    "    print(sql_log)\n",
    "    spark.sql(sql_log)\n",
    "\n",
    "try:\n",
    "    start = datetime.now()\n",
    "    table_name = \"fact_review\"\n",
    "\n",
    "    # Get the size of the `fact_review` table\n",
    "    table_size_mb = get_table_size(\"yelp_analytics.fact_review\")\n",
    "\n",
    "    # Determine Peak/Off-Peak Mode\n",
    "    load_condition = \"peak\" if table_size_mb > PEAK_THRESHOLD_MB else \"off-peak\"\n",
    "\n",
    "    # Adjust Spark settings dynamically\n",
    "    if load_condition == \"peak\":\n",
    "        spark.conf.set(\"spark.sql.shuffle.partitions\", \"500\")  # Allowed runtime change\n",
    "        print(f\"âš¡ Peak Load for {table_name} ({table_size_mb:.2f} MB). High parallelism applied.\")\n",
    "    else:\n",
    "        spark.conf.set(\"spark.sql.shuffle.partitions\", \"100\")  # Allowed runtime change\n",
    "        print(f\"ðŸŒ™ Off-Peak Load for {table_name} ({table_size_mb:.2f} MB). Optimizing resource usage.\")\n",
    "\n",
    "    # Step 1: Read fact_review data from the datamart table\n",
    "    fact_review_df = spark.table(\"yelp_analytics.fact_review\")\n",
    "\n",
    "    # Step 2: Compute review_agg by grouping reviews per business per year\n",
    "    review_agg_df = fact_review_df.withColumn(\"review_year\", year(col(\"review_date\")))\\\n",
    "        .groupBy(\"business_id\", \"review_year\")\\\n",
    "        .agg(count(\"*\").alias(\"review_count\"), avg(\"stars\").alias(\"avg_rating\"))\n",
    "\n",
    "    # Step 3: Self-join review_agg to find rising stars (consecutive years with improvement)\n",
    "    curr_df = review_agg_df.alias(\"curr\")\n",
    "    prev_df = review_agg_df.alias(\"prev\")\n",
    "\n",
    "    rising_stars_df = curr_df.join(\n",
    "        prev_df, \n",
    "        (col(\"curr.business_id\") == col(\"prev.business_id\")) & (col(\"curr.review_year\") == col(\"prev.review_year\") + 1)\n",
    "    ).where(\n",
    "        (col(\"curr.review_count\") >= 10) & (col(\"curr.avg_rating\") >= col(\"prev.avg_rating\") + 1)\n",
    "    ).select(\n",
    "        col(\"curr.business_id\"),\n",
    "        col(\"curr.review_year\").alias(\"current_year\"),\n",
    "        col(\"prev.review_year\").alias(\"previous_year\"),\n",
    "        col(\"curr.review_count\").alias(\"current_review_count\"),\n",
    "        col(\"curr.avg_rating\").alias(\"current_avg\"),\n",
    "        col(\"prev.review_count\").alias(\"prior_review_count\"),\n",
    "        col(\"prev.avg_rating\").alias(\"prior_avg\"),\n",
    "        (col(\"curr.avg_rating\") - col(\"prev.avg_rating\")).alias(\"rating_improvement\"),\n",
    "        concat(lit(\"Rising Star \"), col(\"prev.review_year\").cast(\"string\"), lit(\"-\"), col(\"curr.review_year\").cast(\"string\")).alias(\"rising_star_label\")\n",
    "    )\n",
    "\n",
    "    # Step 4: Join with the business dimension to retrieve the business name\n",
    "    dim_business_df = spark.table(\"yelp_analytics.dim_business\").select(\"business_id\", \"name\")\n",
    "\n",
    "    source_df = rising_stars_df.join(dim_business_df, \"business_id\", \"left\")\\\n",
    "        .select(\n",
    "            \"business_id\",\n",
    "            col(\"name\"),\n",
    "            lit(\"YoY\").alias(\"period_type\"),\n",
    "            to_date(concat(col(\"current_year\").cast(\"string\"), lit(\"-01-01\"))).alias(\"current_period\"),\n",
    "            to_date(concat(col(\"previous_year\").cast(\"string\"), lit(\"-01-01\"))).alias(\"previous_period\"),\n",
    "            array(col(\"rising_star_label\")).alias(\"rising_star_labels\"),\n",
    "            col(\"current_review_count\"),\n",
    "            col(\"current_avg\").alias(\"current_avg_stars\"),\n",
    "            col(\"rating_improvement\").alias(\"current_rating_improvement\"),\n",
    "            col(\"prior_review_count\"),\n",
    "            col(\"prior_avg\").alias(\"prior_avg_stars\"),\n",
    "            lit(None).alias(\"prior_rating_improvement\"),\n",
    "            current_date().alias(\"last_update_date\")\n",
    "        )\n",
    "\n",
    "    # Step 5: Merge the source DataFrame into the target Delta table\n",
    "    target_table = DeltaTable.forName(spark, \"yelp_analytics.datamart_rising_star_businesses\")\n",
    "    merge_condition = \"target.business_id = source.business_id AND target.current_period = source.current_period\"\n",
    "\n",
    "    target_table.alias(\"target\").merge(\n",
    "        source_df.alias(\"source\"),\n",
    "        merge_condition\n",
    "    ).whenMatchedUpdate(\n",
    "        set={\n",
    "            \"name\": \"source.name\",\n",
    "            \"period_type\": \"source.period_type\",\n",
    "            \"previous_period\": \"source.previous_period\",\n",
    "            \"rising_star_labels\": \"source.rising_star_labels\",\n",
    "            \"current_review_count\": \"source.current_review_count\",\n",
    "            \"current_avg_stars\": \"source.current_avg_stars\",\n",
    "            \"current_rating_improvement\": \"source.current_rating_improvement\",\n",
    "            \"prior_review_count\": \"source.prior_review_count\",\n",
    "            \"prior_avg_stars\": \"source.prior_avg_stars\",\n",
    "            \"prior_rating_improvement\": \"source.prior_rating_improvement\",\n",
    "            \"last_update_date\": \"source.last_update_date\"\n",
    "        }\n",
    "    ).whenNotMatchedInsert(\n",
    "        values={\n",
    "            \"business_id\": \"source.business_id\",\n",
    "            \"name\": \"source.name\",\n",
    "            \"period_type\": \"source.period_type\",\n",
    "            \"current_period\": \"source.current_period\",\n",
    "            \"previous_period\": \"source.previous_period\",\n",
    "            \"rising_star_labels\": \"source.rising_star_labels\",\n",
    "            \"current_review_count\": \"source.current_review_count\",\n",
    "            \"current_avg_stars\": \"source.current_avg_stars\",\n",
    "            \"current_rating_improvement\": \"source.current_rating_improvement\",\n",
    "            \"prior_review_count\": \"source.prior_review_count\",\n",
    "            \"prior_avg_stars\": \"source.prior_avg_stars\",\n",
    "            \"prior_rating_improvement\": \"source.prior_rating_improvement\",\n",
    "            \"last_update_date\": \"source.last_update_date\"\n",
    "        }\n",
    "    ).execute()\n",
    "\n",
    "    end = datetime.now()\n",
    "    exec_time = (end - start).total_seconds()\n",
    "\n",
    "    log_elt_process(\"datamart_rising_star_businesses\", \"Success\", execution_time_seconds=exec_time, \n",
    "                    size=f\"{table_size_mb:.2f} MB\", rows_affected=0, method_used=f\"DataFrame ({load_condition.capitalize()})\")\n",
    "\n",
    "except Exception as e:\n",
    "    end = datetime.now()\n",
    "    exec_time = (end - start).total_seconds()\n",
    "    log_elt_process(\"datamart_rising_star_businesses\", \"Failed\", str(e), execution_time_seconds=exec_time, \n",
    "                    size=f\"{table_size_mb:.2f} MB\", rows_affected=0, method_used=f\"DataFrame ({load_condition.capitalize()})\")\n",
    "    raise e\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "YELP - 2.c Data Transformation",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
