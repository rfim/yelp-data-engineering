{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd971043-41a0-4e7f-8f32-16f919761982",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Yelp Data Ingestion\n",
    "\n",
    "\n",
    "<a href=\"https://ibb.co/WWk6nPXQ\"><img src=\"https://i.ibb.co/XZ7bX3P9/ingestion.png\" alt=\"ingestion\" border=\"0\"></a><br /><a target='_blank' href='https://imgbb.com/'>Ingestion Flow Diagram</a><br />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35f23d00-2821-49ec-ac43-3a4bc0339ef4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Convert to **Parquet** for efficient querying and storage.\n",
    "- JSON Data still require Spark context addition, inefficient for OLAP, minimal writes.\n",
    "- **Solution**: Convert to **Parquet** for efficient querying and storage.\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ORHjAuLiy0ksWcQ65Fzepw.png)\n",
    "\n",
    "\n",
    "## Step 2: After ingest, Do Logging & Monitoring\n",
    "- Logs execution time, file size, and records ingested.\n",
    "\n",
    "```python\n",
    "log_sql = f\"\"\"\n",
    "    INSERT INTO config_db.elt_process_log \n",
    "    (log_id, process_name, target_table, start_time, end_time, execution_time_seconds, size, rows_affected, status) \n",
    "    VALUES (last_log_id, 'Data Ingestion', '{table_name}', '{start_time}', '{end_time}', {exec_time}, '{file_size_mb:.2f} MB', {record_count}, 'Success')\n",
    "\"\"\"\n",
    "spark.sql(log_sql)\n",
    "```\n",
    "\n",
    "## Key Learning\n",
    "- JSON is dynamically added to Spark before ingestion.\n",
    "- Parquet improves performance.\n",
    "- Peak mode optimizes large data loads, off-peak mode optimizes resource usage.\n",
    "- Logs support monitoring and optimization.\n",
    "\n",
    "- **Handle Peak vs. Off-Peak Mode**:\n",
    "  - **Peak Mode**: If file size > 1GB, increases parallelism for faster processing.\n",
    "  - **Off-Peak Mode**: Optimized resource usage for smaller files.\n",
    "  \n",
    "```python\n",
    "\n",
    "file_size_mb = os.path.getsize(row.file_path) / (1024 * 1024)\n",
    "    load_condition = \"peak\" if file_size_mb > PEAK_THRESHOLD_MB else \"off-peak\"\n",
    "    \n",
    "    if load_condition == \"peak\":\n",
    "        spark.conf.set(\"spark.sql.shuffle.partitions\", \"500\")\n",
    "        print(f\"‚ö° Peak Load for {table_name} ({file_size_mb:.2f} MB). High parallelism applied.\")\n",
    "    else:\n",
    "        spark.conf.set(\"spark.sql.shuffle.partitions\", \"100\")\n",
    "        print(f\"üåô Off-Peak Load for {table_name} ({file_size_mb:.2f} MB). Optimized resource usage.\")\n",
    "    \n",
    "    df.withColumn(\"ingestion_date\", lit(ingestion_date)).write.mode(\"overwrite\").partitionBy(\"ingestion_date\").parquet(os.path.join(parquet_base_path, table_name))\n",
    "```\n",
    "\n",
    "This ensures **efficient, scalable, and robust** Yelp data processing.\n",
    "\n",
    "The codes run here :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c67aa2e-1bce-485b-b9b4-493c6fede58d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåô Off-Peak Load for business (113.36 MB). Optimized resource usage.\nroot\n |-- address: string (nullable = true)\n |-- attributes: struct (nullable = true)\n |    |-- AcceptsInsurance: string (nullable = true)\n |    |-- AgesAllowed: string (nullable = true)\n |    |-- Alcohol: string (nullable = true)\n |    |-- Ambience: string (nullable = true)\n |    |-- BYOB: string (nullable = true)\n |    |-- BYOBCorkage: string (nullable = true)\n |    |-- BestNights: string (nullable = true)\n |    |-- BikeParking: string (nullable = true)\n |    |-- BusinessAcceptsBitcoin: string (nullable = true)\n |    |-- BusinessAcceptsCreditCards: string (nullable = true)\n |    |-- BusinessParking: string (nullable = true)\n |    |-- ByAppointmentOnly: string (nullable = true)\n |    |-- Caters: string (nullable = true)\n |    |-- CoatCheck: string (nullable = true)\n |    |-- Corkage: string (nullable = true)\n |    |-- DietaryRestrictions: string (nullable = true)\n |    |-- DogsAllowed: string (nullable = true)\n |    |-- DriveThru: string (nullable = true)\n |    |-- GoodForDancing: string (nullable = true)\n |    |-- GoodForKids: string (nullable = true)\n |    |-- GoodForMeal: string (nullable = true)\n |    |-- HairSpecializesIn: string (nullable = true)\n |    |-- HappyHour: string (nullable = true)\n |    |-- HasTV: string (nullable = true)\n |    |-- Music: string (nullable = true)\n |    |-- NoiseLevel: string (nullable = true)\n |    |-- Open24Hours: string (nullable = true)\n |    |-- OutdoorSeating: string (nullable = true)\n |    |-- RestaurantsAttire: string (nullable = true)\n |    |-- RestaurantsCounterService: string (nullable = true)\n |    |-- RestaurantsDelivery: string (nullable = true)\n |    |-- RestaurantsGoodForGroups: string (nullable = true)\n |    |-- RestaurantsPriceRange2: string (nullable = true)\n |    |-- RestaurantsReservations: string (nullable = true)\n |    |-- RestaurantsTableService: string (nullable = true)\n |    |-- RestaurantsTakeOut: string (nullable = true)\n |    |-- Smoking: string (nullable = true)\n |    |-- WheelchairAccessible: string (nullable = true)\n |    |-- WiFi: string (nullable = true)\n |-- business_id: string (nullable = true)\n |-- categories: string (nullable = true)\n |-- city: string (nullable = true)\n |-- hours: struct (nullable = true)\n |    |-- Friday: string (nullable = true)\n |    |-- Monday: string (nullable = true)\n |    |-- Saturday: string (nullable = true)\n |    |-- Sunday: string (nullable = true)\n |    |-- Thursday: string (nullable = true)\n |    |-- Tuesday: string (nullable = true)\n |    |-- Wednesday: string (nullable = true)\n |-- is_open: long (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: double (nullable = true)\n |-- name: string (nullable = true)\n |-- postal_code: string (nullable = true)\n |-- review_count: long (nullable = true)\n |-- stars: double (nullable = true)\n |-- state: string (nullable = true)\n |-- ingestion_date: string (nullable = false)\n\nTotal business records: 150346\n‚úÖ Loaded business with 150346 records from /root/.cache/kagglehub/datasets/yelp-dataset/yelp-dataset/versions/4/yelp_academic_dataset_business.json\n‚úÖ Saved business as Parquet at /data/yelp/parquet/business\nüåô Off-Peak Load for checkin (273.67 MB). Optimized resource usage.\nroot\n |-- business_id: string (nullable = true)\n |-- date: string (nullable = true)\n |-- ingestion_date: string (nullable = false)\n\nTotal checkin records: 131930\n‚úÖ Loaded checkin with 131930 records from /root/.cache/kagglehub/datasets/yelp-dataset/yelp-dataset/versions/4/yelp_academic_dataset_checkin.json\n‚úÖ Saved checkin as Parquet at /data/yelp/parquet/checkin\n‚ö° Peak Load for review (5094.40 MB). High parallelism applied.\n‚ö†Ô∏è NOTE: Adjust `spark.executor.memory` via Spark submit: `--conf spark.executor.memory=16g`\nroot\n |-- business_id: string (nullable = true)\n |-- cool: long (nullable = true)\n |-- date: string (nullable = true)\n |-- funny: long (nullable = true)\n |-- review_id: string (nullable = true)\n |-- stars: double (nullable = true)\n |-- text: string (nullable = true)\n |-- useful: long (nullable = true)\n |-- user_id: string (nullable = true)\n |-- ingestion_date: string (nullable = false)\n\nTotal review records: 6990280\n‚úÖ Loaded review with 6990280 records from /root/.cache/kagglehub/datasets/yelp-dataset/yelp-dataset/versions/4/yelp_academic_dataset_review.json\n‚úÖ Saved review as Parquet at /data/yelp/parquet/review\n‚ö° Peak Load for user (3207.52 MB). High parallelism applied.\n‚ö†Ô∏è NOTE: Adjust `spark.executor.memory` via Spark submit: `--conf spark.executor.memory=16g`\nroot\n |-- average_stars: double (nullable = true)\n |-- compliment_cool: long (nullable = true)\n |-- compliment_cute: long (nullable = true)\n |-- compliment_funny: long (nullable = true)\n |-- compliment_hot: long (nullable = true)\n |-- compliment_list: long (nullable = true)\n |-- compliment_more: long (nullable = true)\n |-- compliment_note: long (nullable = true)\n |-- compliment_photos: long (nullable = true)\n |-- compliment_plain: long (nullable = true)\n |-- compliment_profile: long (nullable = true)\n |-- compliment_writer: long (nullable = true)\n |-- cool: long (nullable = true)\n |-- elite: string (nullable = true)\n |-- fans: long (nullable = true)\n |-- friends: string (nullable = true)\n |-- funny: long (nullable = true)\n |-- name: string (nullable = true)\n |-- review_count: long (nullable = true)\n |-- useful: long (nullable = true)\n |-- user_id: string (nullable = true)\n |-- yelping_since: string (nullable = true)\n |-- ingestion_date: string (nullable = false)\n\nTotal user records: 1987897\n‚úÖ Loaded user with 1987897 records from /root/.cache/kagglehub/datasets/yelp-dataset/yelp-dataset/versions/4/yelp_academic_dataset_user.json\n‚úÖ Saved user as Parquet at /data/yelp/parquet/user\nüåô Off-Peak Load for tip (172.24 MB). Optimized resource usage.\nroot\n |-- business_id: string (nullable = true)\n |-- compliment_count: long (nullable = true)\n |-- date: string (nullable = true)\n |-- text: string (nullable = true)\n |-- user_id: string (nullable = true)\n |-- ingestion_date: string (nullable = false)\n\nTotal tip records: 908915\n‚úÖ Loaded tip with 908915 records from /root/.cache/kagglehub/datasets/yelp-dataset/yelp-dataset/versions/4/yelp_academic_dataset_tip.json\n‚úÖ Saved tip as Parquet at /data/yelp/parquet/tip\nüéâ Data ingestion and Parquet saving completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "spark = SparkSession.builder.appName(\"YelpDataIngestion\").getOrCreate()\n",
    "\n",
    "last_log_id_result = spark.sql(\"SELECT MAX(log_id) FROM config_db.elt_process_log\").collect()\n",
    "last_log_id = last_log_id_result[0][0] if last_log_id_result and last_log_id_result[0][0] is not None else 0\n",
    "\n",
    "source_metadata_df = spark.sql(\"SELECT source_name, file_path, file_format, ingestion_type FROM config_db.source_metadata\")\n",
    "\n",
    "parquet_base_path = \"/data/yelp/parquet/\"\n",
    "ingestion_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Function to get file size in MB\n",
    "def get_file_size(path):\n",
    "    try:\n",
    "        file_size = os.path.getsize(path) / (1024 * 1024)  # Convert bytes to MB\n",
    "        return file_size\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Warning: Unable to retrieve size for {path}. {e}\")\n",
    "        return 0  # Default to 0MB if error occurs\n",
    "\n",
    "# Define threshold (1GB per table) for determining peak vs. off-peak load\n",
    "PEAK_THRESHOLD_MB = 1000  \n",
    "\n",
    "# Process each table dynamically\n",
    "for row in source_metadata_df.collect():\n",
    "    table_name = row.source_name.lower().replace(\" \", \"_\")  \n",
    "    file_format = row.file_format.lower()\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    sc = spark.sparkContext\n",
    "    sc.addFile(row.file_path)\n",
    "    \n",
    "    local_path = SparkFiles.get(row.file_path.split('/')[-1])\n",
    "    spark_read_path = f\"file://{local_path}\"\n",
    "\n",
    "    file_size_mb = get_file_size(local_path)\n",
    "    \n",
    "    load_condition = \"peak\" if file_size_mb > PEAK_THRESHOLD_MB else \"off-peak\"\n",
    "\n",
    "    if load_condition == \"peak\":\n",
    "        spark.conf.set(\"spark.sql.shuffle.partitions\", \"500\")  # Allowed runtime change\n",
    "        print(f\"‚ö° Peak Load for {table_name} ({file_size_mb:.2f} MB). High parallelism applied.\")\n",
    "        print(\"‚ö†Ô∏è NOTE: Adjust `spark.executor.memory` via Spark submit: `--conf spark.executor.memory=16g`\")\n",
    "    else:\n",
    "        spark.conf.set(\"spark.sql.shuffle.partitions\", \"100\")  # Allowed runtime change\n",
    "        print(f\"üåô Off-Peak Load for {table_name} ({file_size_mb:.2f} MB). Optimized resource usage.\")\n",
    "\n",
    "    # Read data based on format\n",
    "    if file_format == \"json\":\n",
    "        df = spark.read.json(spark_read_path)\n",
    "    elif file_format == \"csv\":\n",
    "        df = spark.read.option(\"header\", \"true\").csv(spark_read_path)\n",
    "    elif file_format == \"parquet\":\n",
    "        df = spark.read.parquet(spark_read_path)\n",
    "    else:\n",
    "        print(f\"‚ùå Unsupported file format: {file_format} for {table_name}\")\n",
    "        continue\n",
    "    \n",
    "    df = df.withColumn(\"ingestion_date\", lit(ingestion_date))\n",
    "    \n",
    "    df.printSchema()\n",
    "    record_count = df.count()\n",
    "    print(f\"Total {table_name} records: {record_count}\")\n",
    "    print(f\"‚úÖ Loaded {table_name} with {record_count} records from {row.file_path}\")\n",
    "\n",
    "    # Save as Parquet\n",
    "    parquet_save_path = os.path.join(parquet_base_path, table_name)\n",
    "    df.write.mode(\"overwrite\").partitionBy(\"ingestion_date\").parquet(parquet_save_path)\n",
    "    print(f\"‚úÖ Saved {table_name} as Parquet at {parquet_save_path}\")\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    exec_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "    # Increment log_id for this process\n",
    "    last_log_id += 1\n",
    "\n",
    "    # Log with Peak/Off-Peak Mode\n",
    "    log_sql = f\"\"\"\n",
    "        INSERT INTO config_db.elt_process_log \n",
    "        (log_id, process_name, target_table, start_time, end_time, execution_time_seconds, size, rows_affected, method_used, status, error_message) \n",
    "        VALUES ({last_log_id}, 'Refresh Data Ingestion', '{table_name}', '{start_time}', '{end_time}', {exec_time}, '{file_size_mb:.2f} MB', {record_count}, 'DataFrame ({load_condition.capitalize()})', 'Success', NULL)\n",
    "    \"\"\"\n",
    "    spark.sql(log_sql)\n",
    "\n",
    "print(\"üéâ Data ingestion and Parquet saving completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d26a52fd-e6c5-42ef-aeaa-4e57c05188b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3604340166488990,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "YELP - 2.a Data Ingestion",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
